<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Seedwork: A New Methodology for Building with AI - Seedwork</title>
    <meta name="description" content="Why everything I learned about software development is both right and wrong.">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <nav>
        <div class="container">
            <a href="/" class="site-name">Seedwork</a>
            <ul>
                <li><a href="/">Manifesto</a></li>
                <li><a href="/seeds/">Seeds</a></li>
                <li><a href="/about.html">About</a></li>
            </ul>
        </div>
    </nav>

    <main class="container">
        <h1>Seedwork: A New Methodology for Building with AI</h1>
        <p class="subtitle">Why everything I learned about software development is both right and wrong</p>

        <hr>

        <p>I'm old enough to remember when extreme programming, the predecessor to agile, was fringe, ground upon, and scorned in the corporation.</p>

        <p>I started my career in waterfall. Thick specification documents. Change control boards. The belief that if you just thought hard enough upfront, you could anticipate well. Then I adopted Extreme Programming and it felt radical—and it felt right.</p>

        <p>Agile won. The core advance was simple: requirements change, so embrace change. Stop pretending you can know everything upfront. Iterate, ship, learn.</p>

        <p>That insight served us for twenty years. And now it needs to evolve again.</p>

        <hr>

        <h2>What Agile Got Right (And What It Assumed)</h2>

        <p>Agile solved the requirements problem. It acknowledged that users don't know what they want until they see it, that markets shift, that the world changes. It gave us tools for adapting: sprints, retrospectives, continuous delivery.</p>

        <p>But agile assumed something that seemed obvious at the time: <strong>the software itself is deterministic</strong>. You write code, it runs the same way every time. The inputs and outputs are predictable. The system does precisely what you programmed it to do—no more, no less.</p>

        <p>When you build with large language models, that assumption is broken.</p>

        <hr>

        <h2>What Breaks When the System Isn't Deterministic</h2>

        <p>A Large Language Model (LLM)-based system isn't likely to do exactly the same thing twice. Give it the same input, you should get slightly different outputs. That sounds like a bug. It's actually the core value.</p>

        <p>I've spent the past year learning how we can build AI-native applications for mission-driven organizations. And I keep having the same experience: <strong>the most valuable thing the system does is something I didn't plan for.</strong></p>

        <p>Agile gave us user stories, acceptance criteria, definition of done. But what's the acceptance criterion when the system might do something different—and better—than you specified?</p>

        <p>Traditional software engineering asks: "Does it meet the requirements?"<br>
        LLM-native development asks: "What else will it do?"</p>

        <p>The shift is fundamental:</p>

        <table>
            <thead>
                <tr>
                    <th>Agile Assumption</th>
                    <th>LLM Reality</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>System does what you specify</td>
                    <td>System has latent capabilities you discover</td>
                </tr>
                <tr>
                    <td>Same input → same output</td>
                    <td>Same input → variable output</td>
                </tr>
                <tr>
                    <td>Test against fixed expectations</td>
                    <td>Evaluate in context; success may surprise you</td>
                </tr>
                <tr>
                    <td>Build features</td>
                    <td>Cultivate conditions</td>
                </tr>
                <tr>
                    <td>Control the system</td>
                    <td>Work with the grain</td>
                </tr>
            </tbody>
        </table>

        <p>Martin Fowler <a href="https://martinfowler.com/articles/202508-ai-thoughts.html">put it bluntly</a>, describing the thoughts of his colleague Rebecca Parsons: "All an LLM does is produce hallucinations, it's just that we find some of them useful." Kent Beck describes "<a href="https://tidyfirst.substack.com/p/programming-deflation">programming deflation</a>"—a world where "the cost of 'what if we tried...' approaches zero." His advice: don't predict which future we'll get. Cultivate capabilities that thrive in either scenario.</p>

        <hr>

        <h2>Seedwork: Cultivation Over Construction</h2>

        <p>The AI, Claude, named the methodology we need <strong>Seedwork</strong>.</p>

        <p>A seed contains latent potential. You don't build a plant—you provide conditions for growth. You work with species that thrive in a particular environment. The gardener's job isn't to control growth, but to cultivate it.</p>

        <p>We need a similar methodology when building with LLMs. Our work is cultivation: providing the right context, the right tools, the right connection to problems. We plant seeds, create conditions, and then—critically—pay attention to what grows.</p>

        <p>The core values:</p>

        <ul>
            <li><strong>Emergent capabilities</strong> over specified requirements</li>
            <li><strong>Cultivation</strong> over construction</li>
            <li><strong>Discovery through use</strong> over upfront design</li>
            <li><strong>Native strengths</strong> over forced behaviors</li>
        </ul>

        <p>The metaphor matters. "Software engineering" implies construction—blueprints, building, precise specifications. Seedwork implies something else: you're working with systems that have their own tendencies. The job is to notice what thrives and nurture it.</p>

        <hr>

        <h2>What This Means for Mission-Driven Organizations</h2>

        <p>I have spent my career working primarily with nonprofits and social enterprises. These organizations are asking: "Should we be investing in AI?"</p>

        <p>In the past, applying AI required data science teams, infrastructure, expertise. It was enterprise-grade or nothing.</p>

        <p>That's changed. The new tools—Claude, GPT-4, open-source models—have facilitated access to AI. A small team can now build things that would have required a department five years ago.</p>

        <p>But here's what matters for mission-driven work: <strong>Seedwork favors organizations that know their problems deeply.</strong></p>

        <p>Traditional software development rewards specification precision. Can you write detailed requirements? Do you have a product manager who can think through edge cases?</p>

        <p>Seedwork rewards something different: <strong>Can you recognize unexpected value when you see it?</strong></p>

        <p>Organizations with deep domain expertise are ideally positioned for Seedwork. You don't need to specify everything upfront. You need to recognize what's growing and know which aspects are valuable.</p>

        <hr>

        <h2>The Transition</h2>

        <p>I've lived through two of these transitions now. Waterfall to agile felt like a very exciting conversion. We had to unlearn the belief that more planning was always better. We had to trust iteration, trust the process of discovering requirements through building.</p>

        <p>Agile to Seedwork feels similar but different. We still iterate. We still ship early and often and learn and adopt. But now we're also watching for emergence. We're asking not just "Does it work?" but "What else does it do?"</p>

        <p>The skills transfer. Agile developers who learned to embrace changing requirements can learn to embrace emergent capabilities. The mindset of curiosity, of shipping to learn, of staying close to users—all of that remains valuable.</p>

        <p>What's new is the relationship to the system. It's not a tool anymore. It's closer to a collaborator—one with latent capabilities you're still discovering.</p>

        <hr>

        <h2>Getting Started</h2>

        <p>If you're considering building something with AI, a few practical thoughts:</p>

        <p><strong>Start with a real problem you understand deeply.</strong> The best Seedwork happens when you can recognize unexpected value because you know the domain. Generic AI experiments rarely reveal much.</p>

        <p><strong>Ship early to real users.</strong> You cannot discover emergent capabilities in a sandbox. The magic happens when the system meets actual human needs in unpredictable ways.</p>

        <p><strong>Pay attention to surprises.</strong> When the system does something you didn't plan for, don't immediately "fix" it. Ask first: is this valuable? Some of your best features will be discovered, not designed.</p>

        <p><strong>Work with the grain.</strong> Notice what the AI naturally does well and lean into that. Don't fight the model's tendencies—cultivate the valuable ones.</p>

        <p><strong>Keep the scaffolding minimal.</strong> The intelligence is in the model. Your code connects it to context and action. Resist the urge to over-engineer.</p>

        <hr>

        <h2>The World Isn't Flat Anymore</h2>

        <p>Waterfall gave us discipline. Agile gave us adaptability. Seedwork gives us something new: a methodology for working with systems that surprise us. For cultivating emergence rather than controlling execution. For building software that might, if we pay attention, do something better than we imagined.</p>

        <p>The seeds are planted. What grows next is up to us—and partly up to them.</p>

        <footer>
            <p><em><a href="/about.html">Michael Knapp</a>, in collaboration with Claude (Anthropic)</em></p>
            <p><a href="/seeds/">← Back to Seeds</a></p>
        </footer>
    </main>
</body>
</html>
